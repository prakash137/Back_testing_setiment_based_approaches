{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed steps to develop a portfolio optimization model that incorporates sentiment-enhanced features. The notebook will cover:\n",
    "\n",
    "1. **Data Collection**: Collecting financial and sentiment data.\n",
    "2. **Sentiment Analysis**: Using a fine-tuned BERT model for sentiment scoring.\n",
    "3. **Portfolio Optimization**: Implementing both classical Mean-Variance Optimization and advanced Black-Litterman model.\n",
    "4. **Constraints and Objectives**: Applying constraints and objectives in the optimization process.\n",
    "\n",
    "Here is the content for the Jupyter Notebook:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Portfolio Optimization with Sentiment Analysis\n",
    "\n",
    "#### 1. Install Necessary Libraries\n",
    "\n",
    "Make sure to install the required libraries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "!pip install transformers datasets yfinance cvxpy stable-baselines3 nltk\n",
    "!pip install gym\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pra/Projects /Sentiment Analysis/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, pipeline\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcvxpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Variable, Maximize, quad_form, Problem, \u001b[38;5;28msum\u001b[39m, Parameter\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_env\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 2. Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from cvxpy import Variable, Maximize, quad_form, Problem, sum, Parameter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.envs import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. Load and Fine-Tune BERT Model (if not already done)\n",
    "\n",
    "You can skip this section if you already have a fine-tuned BERT model. Otherwise, use the following code to fine-tune and save the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Prepare for PyTorch\n",
    "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Fine-tune BERT\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('./fine-tuned-bert')\n",
    "tokenizer.save_pretrained('./fine-tuned-bert')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 4. Load the Fine-Tuned BERT Model\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./fine-tuned-bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine-tuned-bert')\n",
    "\n",
    "# Create sentiment analysis pipeline\n",
    "sentiment_model = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 5. Collect Financial Data\n",
    "\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "data = yf.download(tickers, start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 6. Collect and Process Sentiment Data\n",
    "\n",
    "def get_news_data(ticker):\n",
    "    url = (f'https://newsapi.org/v2/everything?'\n",
    "           f'q={ticker}&'\n",
    "           'from=2020-01-01&'\n",
    "           'sortBy=popularity&'\n",
    "           'apiKey=YOUR_NEWSAPI_KEY')\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "# Example for one stock\n",
    "news_data = get_news_data(\"AAPL\")\n",
    "\n",
    "# Extract and preprocess news articles\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "news_articles = [preprocess_text(article['description']) for article in news_data['articles']]\n",
    "\n",
    "# Get sentiment scores\n",
    "sentiment_scores = sentiment_model(news_articles)\n",
    "\n",
    "# Compute sentiment index\n",
    "def compute_sentiment_index(sentiment_scores):\n",
    "    positive_scores = [score['score'] for score in sentiment_scores if score['label'] == 'POSITIVE']\n",
    "    negative_scores = [score['score'] for score in sentiment_scores if score['label'] == 'NEGATIVE']\n",
    "    return sum(positive_scores) - sum(negative_scores)\n",
    "\n",
    "sentiment_index = compute_sentiment_index(sentiment_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 7. Black-Litterman Model\n",
    "\n",
    "# Calculate returns and covariance matrix\n",
    "returns = data['Adj Close'].pct_change().dropna()\n",
    "mu = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "\n",
    "# Black-Litterman parameters\n",
    "tau = 0.025  # scaling factor\n",
    "P = np.eye(len(tickers))  # Identity matrix for simplicity\n",
    "Q = np.array([sentiment_index] * len(tickers))  # Sentiment views\n",
    "\n",
    "# Calculate Black-Litterman expected returns\n",
    "pi = np.dot(cov_matrix, mu)\n",
    "sigma_inv = np.linalg.inv(cov_matrix)\n",
    "omega = np.diag(np.diag(np.dot(np.dot(P, cov_matrix), P.T)) * tau)\n",
    "bl_returns = np.linalg.inv(sigma_inv + np.dot(np.dot(P.T, np.linalg.inv(omega)), P))\n",
    "bl_returns = np.dot(bl_returns, (np.dot(sigma_inv, pi) + np.dot(np.dot(P.T, np.linalg.inv(omega)), Q)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 8. Portfolio Optimization with Constraints\n",
    "\n",
    "\n",
    "n = len(tickers)\n",
    "w = Variable(n)\n",
    "gamma = Parameter(nonneg=True)  # Risk aversion coefficient\n",
    "\n",
    "# Constraints\n",
    "constraints = [\n",
    "    sum(w) == 1,         # Weights must sum to 1\n",
    "    w >= 0,              # No short selling\n",
    "    # Additional constraints such as sector diversification and liquidity can be added here\n",
    "]\n",
    "\n",
    "# Objective function\n",
    "objective = Maximize(bl_returns.T @ w - gamma * quad_form(w, cov_matrix))\n",
    "\n",
    "# Optimization problem\n",
    "problem = Problem(objective, constraints)\n",
    "\n",
    "# Solve for different levels of risk aversion\n",
    "gamma_values = np.logspace(-2, 2, num=50)\n",
    "portfolio_weights = []\n",
    "\n",
    "for gamma_value in gamma_values:\n",
    "    gamma.value = gamma_value\n",
    "    problem.solve()\n",
    "    portfolio_weights.append(w.value)\n",
    "\n",
    "# Convert to DataFrame\n",
    "portfolio_weights_df = pd.DataFrame(portfolio_weights, columns=tickers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 9. Evaluate Portfolio Performance\n",
    "\n",
    "\n",
    "# Calculate expected returns and risks for each portfolio\n",
    "expected_returns = portfolio_weights_df @ bl_returns\n",
    "risks = [np.sqrt(w.T @ cov_matrix @ w) for w in portfolio_weights]\n",
    "\n",
    "# Sharpe ratio\n",
    "risk_free_rate = 0.01  # Assuming a risk-free rate of 1%\n",
    "sharpe_ratios = (expected_returns - risk_free_rate) / risks\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(risks, expected_returns, 'o-')\n",
    "for i, txt in enumerate(tickers):\n",
    "    plt.annotate(txt, (risks[i], expected_returns[i]))\n",
    "plt.xlabel('Risk (Standard Deviation)')\n",
    "plt.ylabel('Expected Return')\n",
    "plt.title('Efficient Frontier with Black-Litterman Model')\n",
    "plt.show()\n",
    "\n",
    "# Plot Sharpe ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gamma_values, sharpe_ratios, 'o-')\n",
    "plt.xlabel('Risk Aversion Coefficient (gamma)')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.title('Sharpe Ratio vs Risk Aversion')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### 10. Reinforcement Learning\n",
    "\n",
    "\n",
    "from gym import Env, spaces\n",
    "\n",
    "class PortfolioEnv(Env):\n",
    "    def __init__(self, returns, cov_matrix, bl_returns, risk_free_rate):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.returns = returns\n",
    "        self.cov_matrix = cov_matrix\n",
    "        self.bl_returns = bl_returns\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.num_assets = returns.shape[1]\n",
    "        \n",
    "        # Action space: asset weights\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.num_assets,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: returns, covariance matrix, and Black-Litterman expected returns\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_assets + self.num_assets**2 + self.num_assets,), dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset state to initial observation\n",
    "        self.state = np.concatenate([self.bl_returns, self.cov_matrix.flatten(), self.returns.mean(axis=0)])\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Normalize action to ensure weights sum to 1\n",
    "        weights = action / np.sum(action)\n",
    "        \n",
    "        # Calculate portfolio return and risk\n",
    "        portfolio_return = np.dot(weights, self.bl_returns)\n",
    "        portfolio_risk = np.sqrt(np.dot\n",
    "\n",
    "(weights.T, np.dot(self.cov_matrix, weights)))\n",
    "        \n",
    "        # Calculate Sharpe ratio as the reward\n",
    "        sharpe_ratio = (portfolio_return - self.risk_free_rate) / portfolio_risk\n",
    "        \n",
    "        # For simplicity, consider each step as a terminal step\n",
    "        done = True\n",
    "        \n",
    "        # Calculate next state (not relevant in this terminal step setting)\n",
    "        self.state = np.concatenate([self.bl_returns, self.cov_matrix.flatten(), self.returns.mean(axis=0)])\n",
    "        \n",
    "        return self.state, sharpe_ratio, done, {}\n",
    "\n",
    "# Create and check environment\n",
    "env = PortfolioEnv(returns, cov_matrix, bl_returns, risk_free_rate)\n",
    "check_env(env)\n",
    "\n",
    "# Train RL model\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Get optimized portfolio weights\n",
    "obs = env.reset()\n",
    "action, _states = model.predict(obs)\n",
    "optimized_weights = action / np.sum(action)\n",
    "print(\"Optimized Portfolio Weights:\", optimized_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
